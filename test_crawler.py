#!/usr/bin/env python3
"""
Test script for the improved Takaichi website crawler.
"""

import sys
import json
from pathlib import Path
from collections import Counter

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from config import SCRAPER_CONFIG, RAW_DATA_DIR
from scraper import WebCrawler


def test_crawler():
    """Test the improved web crawler."""
    print("=" * 60)
    print("é«˜å¸‚æ—©è‹—ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    # Initialize crawler
    crawler = WebCrawler(SCRAPER_CONFIG, RAW_DATA_DIR)

    # Run the crawler
    scraped_data = crawler.crawl_all_pages()

    # Analyze results
    print("\n" + "=" * 60)
    print("ğŸ“Š ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°çµæœåˆ†æ")
    print("=" * 60)

    # Count pages by category
    category_counts = Counter(item['category'] for item in scraped_data)

    print(f"\nç·ãƒšãƒ¼ã‚¸æ•°: {len(scraped_data)}")
    print("\nã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒšãƒ¼ã‚¸æ•°:")
    for category, count in category_counts.items():
        print(f"  - {category}: {count} ãƒšãƒ¼ã‚¸")

    # Analyze URLs
    print("\nURL ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
    url_patterns = Counter()
    for item in scraped_data:
        url = item['url']
        if 'column_detail' in url:
            url_patterns['column_detail'] += 1
        elif 'column_list' in url:
            url_patterns['column_list'] += 1
        elif 'kaiken_detail' in url:
            url_patterns['kaiken_detail'] += 1
        elif 'kaiken_list' in url:
            url_patterns['kaiken_list'] += 1
        elif 'results_' in url:
            url_patterns['results_pages'] += 1
        else:
            url_patterns['main_pages'] += 1

    for pattern, count in url_patterns.items():
        print(f"  - {pattern}: {count} ãƒšãƒ¼ã‚¸")

    # Calculate total word count
    total_words = sum(item['word_count'] for item in scraped_data)
    avg_words = total_words // len(scraped_data) if scraped_data else 0

    print(f"\nç·å˜èªæ•°: {total_words:,}")
    print(f"å¹³å‡å˜èªæ•°/ãƒšãƒ¼ã‚¸: {avg_words:,}")

    # Save detailed report
    report_path = RAW_DATA_DIR / "crawler_test_report.json"
    report = {
        "total_pages": len(scraped_data),
        "category_counts": dict(category_counts),
        "url_patterns": dict(url_patterns),
        "total_words": total_words,
        "average_words_per_page": avg_words,
        "sample_pages": [
            {
                "url": item['url'],
                "title": item['title'][:50] + "..." if len(item['title']) > 50 else item['title'],
                "category": item['category'],
                "word_count": item['word_count']
            }
            for item in scraped_data[:10]  # First 10 pages as sample
        ]
    }

    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å…ˆ: {report_path}")

    # Save scraped data
    data_path = crawler.save_data()
    print(f"ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ: {data_path}")

    # Success criteria
    print("\n" + "=" * 60)
    print("âœ… ãƒ†ã‚¹ãƒˆçµæœ")
    print("=" * 60)

    success_criteria = [
        ("åŸºæœ¬ç†å¿µ (idea) ãƒšãƒ¼ã‚¸å–å¾—", category_counts.get('idea', 0) >= 1),
        ("æ”¿æ²»å§¿å‹¢ (posture) ãƒšãƒ¼ã‚¸å–å¾—", category_counts.get('posture', 0) >= 1),
        ("å®Ÿç¸¾ (results) ãƒšãƒ¼ã‚¸å–å¾—", category_counts.get('results', 0) >= 2),
        ("è¨˜è€…ä¼šè¦‹ (kaiken) è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—", url_patterns.get('kaiken_detail', 0) >= 10),
        ("ã‚³ãƒ©ãƒ  (column) è©³ç´°ãƒšãƒ¼ã‚¸å–å¾—", url_patterns.get('column_detail', 0) >= 10),
        ("ç·ãƒšãƒ¼ã‚¸æ•° > 50", len(scraped_data) > 50),
    ]

    all_passed = True
    for criteria, passed in success_criteria:
        status = "âœ…" if passed else "âŒ"
        print(f"{status} {criteria}")
        if not passed:
            all_passed = False

    if all_passed:
        print("\nğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã«åˆæ ¼ã—ã¾ã—ãŸï¼")
        print("ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
    else:
        print("\nâš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        print("ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    return scraped_data


if __name__ == "__main__":
    test_crawler()